{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = {(4,0):4,(4,1):1,(0,1):3,(1,3):2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flags:\n",
    "\n",
    "    def __init__(self):\n",
    "        #### any info you want to store are here, for instance:\n",
    "        self.final_state = \n",
    "        self.flags = \n",
    "        ## hint: self.flags indicate the position of each flag and the correct order it should be collected\n",
    "    \n",
    "    \n",
    "    def initial_state(self):\n",
    "    # return the initial state of this MDP ...\n",
    "\n",
    "        \n",
    "    def get_all_states(self):\n",
    "    # return all the possible states in this MDP\n",
    "\n",
    "    \n",
    "    def is_terminal (self, state) :\n",
    "    # this function should return a Boolean indicating whether or not state is a terminal state; in other \n",
    "    # words , does the game end at the specified state or does the robot keep playing ?\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def transition(self, state , action) :\n",
    "    # this function should simulate the intended behavior of the Flags domain\n",
    "    # in particular , given the specified state and action, this function should return a tuple containing two things :\n",
    "    # 1. the next state to which we are transitioning to in the next period\n",
    "    # 2. the reward obtained according to your rewardfunction when transitioning to the next state\n",
    "    ## don't need to return the whole transition matrix\n",
    "    ## also, if the current state is the final state, we stop\n",
    "\n",
    "\n",
    "\n",
    "        return next_state, reward\n",
    "    \n",
    "    \n",
    "    #### you can include any other helper functions you want\n",
    "    ## for instance: a helper function to see if the collection happens or not (if a collection satisify the collection order...)\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I only provide the structure following the pdf innstructions for SARSA\n",
    "Other three algorithms can follow the same structure \n",
    "\n",
    "Again, feel free to include/add any functions you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA:\n",
    "    \n",
    "    ## initalize with all the parameters you have (discount factor, learning rate, ....)\n",
    "    ## also, you can include the flag domain you are solving as part of SARSA object if you want (self.flag = input_flag)\n",
    "    def __init__(self,...):\n",
    "        # initialize the domain,epsilon and discount factor, lr... (you may also need to store Q here, also actions....)\n",
    "\n",
    "        \n",
    "        \n",
    "    def initialize_values(self):\n",
    "        # you need to initialize Q = 0 for all state action pairs\n",
    "        # implement Q as a dictionary is recommended - this will guarantee constant-time reading and writing\n",
    "\n",
    "\n",
    "    \n",
    "    #### used to get soft-greedy action\n",
    "    def sample_epsilon_greedy(self,state):\n",
    "#     takes a state of the MDP as argu- ment and returns an action sampled according to the epsilon-greedy policy defined\n",
    "#     by the epsilon parameter set in the init method and the Q-values, returns the action selected\n",
    "\n",
    "        return action_selected\n",
    "    \n",
    "\n",
    " \n",
    "    def sample_greedy(self, state):\n",
    "        # greedy policy\n",
    "\n",
    "        return greedy_action\n",
    "    \n",
    "    def test_greedy(self,episode_len=200):\n",
    "\n",
    "        ## you can return anything you want to monitor the performances of testing\n",
    "    \n",
    "    \n",
    "    #### you need to define the main training loop and helper functionns yourself"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
