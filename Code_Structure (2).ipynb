{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = {(4,0):4,(4,1):1,(0,1):3,(1,3):2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flags:\n",
    "\n",
    "    def __init__(self, flags):\n",
    "        #### any info you want to store are here, for instance:\n",
    "        self.final_state = (4,4)\n",
    "        # Sort given flag positions into ordered-dictionary by flag value\n",
    "        self.flagPos = flags\n",
    "        self.flags = list({k: v for k, v in sorted(flags.items(), key=lambda item: item[1])}.keys())\n",
    "        ## hint: self.flags indicate the position of each flag and the correct order it should be collected\n",
    "        self.flagCounter = 0\n",
    "        self.action_set = {(0,1),(0,-1),(-1,0),(1,0)}\n",
    "    \n",
    "    def initial_state(self):\n",
    "    # return the initial state of this MDP ...\n",
    "        return (0,0)\n",
    "\n",
    "    def get_all_states(self):\n",
    "    # return all the possible states in this MDP\n",
    "        return [(x,y) for x in range(5) for y in range(5)]\n",
    "    \n",
    "    def is_terminal (self, state) :\n",
    "    # this function should return a Boolean indicating whether or not state is a terminal state; in other \n",
    "    # words , does the game end at the specified state or does the robot keep playing ?\n",
    "        if state == (4,4):\n",
    "            return True\n",
    "        else:\n",
    "            return False    \n",
    "    \n",
    "    def transition(self, state , action) :\n",
    "    # this function should simulate the intended behavior of the Flags domain\n",
    "    # in particular ,t given the specified state and action, his function should return a tuple containing two things :\n",
    "    # 1. the next state to which we are transitioning to in the next period\n",
    "    # 2. the reward obtained according to your rewardfunction when transitioning to the next state\n",
    "    ## don't need to return the whole transition matrix\n",
    "    ## also, if the current state is the final state, we stop\n",
    "\n",
    "    # Denote actions from set {(0,1),(0,-1),(-1,0),(1,0)} as {up,down,left,right}\n",
    "        # Check if current state is already terminal\n",
    "        if self.is_terminal(state):\n",
    "            return state, 1\n",
    "        # Generate next state\n",
    "        next_state = self.computeNextState(state, action)\n",
    "        # Check if next state is terminal\n",
    "        if self.is_terminal(next_state):\n",
    "            return next_state, 1\n",
    "        # Check if flag is in remaining-flags-list\n",
    "        if next_state in self.flags:\n",
    "            # Check if it's the first flag\n",
    "            if next_state == self.flags[0]:\n",
    "                # Capture flag, update flags array\n",
    "                self.flags = self.flags[1:]\n",
    "                return next_state, 1\n",
    "            # Not the first flag, penalize agent\n",
    "            else:\n",
    "                return next_state, -3\n",
    "        # Normal move, continue\n",
    "        else:\n",
    "            return next_state, -1\n",
    "    \n",
    "    def computeNextState(self, state, action):\n",
    "        # Assert action is valid\n",
    "        try:\n",
    "            assert action in self.action_set\n",
    "        except:\n",
    "            print(\"Illegal action given!\")\n",
    "        # Check if agent would go out-of-bounds\n",
    "        if self.isOutOfBounds(state, action):\n",
    "            return state\n",
    "        else:\n",
    "            return (state[0]+action[0],state[1]+action[1])\n",
    "\n",
    "    def isOutOfBounds(self, state, action):\n",
    "        if (state[0] == 0 and action == (-1,0)) or \\\n",
    "            (state[0] == 4 and action == (1,0)) or \\\n",
    "            (state[1] == 0 and action == (0,-1)) or \\\n",
    "            (state[1] == 4 and action == (0,1)):\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I only provide the structure following the pdf innstructions for SARSA\n",
    "Other three algorithms can follow the same structure \n",
    "\n",
    "Again, feel free to include/add any functions you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA:\n",
    "    \n",
    "    ## initalize with all the parameters you have (discount factor, learning rate, ....)\n",
    "    ## also, you can include the flag domain you are solving as part of SARSA object if you want (self.flag = input_flag)\n",
    "    def __init__(self,...):\n",
    "        # initialize the domain,epsilon and discount factor, lr... (you may also need to store Q here, also actions....)\n",
    "\n",
    "        \n",
    "        \n",
    "    def initialize_values(self):\n",
    "        # you need to initialize Q = 0 for all state action pairs\n",
    "        # implement Q as a dictionary is recommended - this will guarantee constant-time reading and writing\n",
    "\n",
    "\n",
    "    \n",
    "    #### used to get soft-greedy action\n",
    "    def sample_epsilon_greedy(self,state):\n",
    "#     takes a state of the MDP as argu- ment and returns an action sampled according to the epsilon-greedy policy defined\n",
    "#     by the epsilon parameter set in the init method and the Q-values, returns the action selected\n",
    "\n",
    "        return action_selected\n",
    "    \n",
    "\n",
    " \n",
    "    def sample_greedy(self, state):\n",
    "        # greedy policy\n",
    "\n",
    "        return greedy_action\n",
    "    \n",
    "    def test_greedy(self,episode_len=200):\n",
    "\n",
    "        ## you can return anything you want to monitor the performances of testing\n",
    "    \n",
    "    \n",
    "    #### you need to define the main training loop and helper functionns yourself"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
